#!/usr/bin/env python3
"""
Phase 1 Final Validation and Performance Test
============================================

Comprehensive end-to-end testing of the integrated system to validate
Phase 1 completion and assess production readiness.

Tests:
1. System startup and initialization
2. Component integration validation
3. Application functionality verification
4. Performance benchmarking
5. Error handling and recovery
6. Security assessment
7. Production readiness evaluation

Author: GitHub Copilot
Date: 2024
"""

import os
import sys
import time
import json
import asyncio
import threading
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
import concurrent.futures

class Phase1Validator:
    """Comprehensive Phase 1 validation test suite"""
    
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "phase": "Phase 1 - Final Validation",
            "tests": [],
            "performance": {},
            "security": {},
            "summary": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "warnings": 0
            }
        }
    
    def log_test(self, test_name: str, status: str, message: str = "", details: Dict = None):
        """Log test result"""
        test_result = {
            "name": test_name,
            "status": status,
            "message": message,
            "details": details or {},
            "timestamp": datetime.now().isoformat()
        }
        self.results["tests"].append(test_result)
        self.results["summary"]["total_tests"] += 1
        
        if status == "PASS":
            self.results["summary"]["passed"] += 1
            print(f"‚úÖ {test_name}: {message}")
        elif status == "FAIL":
            self.results["summary"]["failed"] += 1
            print(f"‚ùå {test_name}: {message}")
        elif status == "WARN":
            self.results["summary"]["warnings"] += 1
            print(f"‚ö†Ô∏è  {test_name}: {message}")
    
    def test_startup_performance(self):
        """Test system startup performance"""
        print("\nüöÄ Testing Startup Performance...")
        
        # Test configuration loading time
        start_time = time.time()
        try:
            from integrated_config import LangGraphIntegratedConfig
            config = LangGraphIntegratedConfig()
            config_time = time.time() - start_time
            
            if config_time < 1.0:
                self.log_test("Configuration Loading Speed", "PASS", 
                            f"Loaded in {config_time:.3f}s", {"load_time": config_time})
            else:
                self.log_test("Configuration Loading Speed", "WARN", 
                            f"Slow loading: {config_time:.3f}s", {"load_time": config_time})
        except Exception as e:
            self.log_test("Configuration Loading Speed", "FAIL", str(e))
        
        # Test integration adapter startup
        start_time = time.time()
        try:
            from langgraph_integration_adapter import LangGraphIntegrationAdapter
            adapter = LangGraphIntegrationAdapter()
            adapter_time = time.time() - start_time
            
            if adapter_time < 2.0:
                self.log_test("Integration Adapter Startup", "PASS", 
                            f"Started in {adapter_time:.3f}s", {"startup_time": adapter_time})
            else:
                self.log_test("Integration Adapter Startup", "WARN", 
                            f"Slow startup: {adapter_time:.3f}s", {"startup_time": adapter_time})
        except Exception as e:
            self.log_test("Integration Adapter Startup", "FAIL", str(e))
    
    def test_application_integration(self):
        """Test that applications integrate correctly"""
        print("\nüîó Testing Application Integration...")
        
        # Test enhanced CLI availability
        try:
            from cli_integration_patch import patch_cli_app
            cli_patch = patch_cli_app()
            if cli_patch:
                self.log_test("CLI Integration", "PASS", "CLI patch available and functional")
            else:
                self.log_test("CLI Integration", "WARN", "CLI patch in fallback mode")
        except Exception as e:
            self.log_test("CLI Integration", "FAIL", str(e))
        
        # Test enhanced Streamlit availability
        try:
            from streamlit_integration_patch import patch_streamlit_app
            streamlit_patch = patch_streamlit_app()
            if streamlit_patch:
                self.log_test("Streamlit Integration", "PASS", "Streamlit patch available")
            else:
                self.log_test("Streamlit Integration", "WARN", "Streamlit patch in fallback mode")
        except Exception as e:
            self.log_test("Streamlit Integration", "FAIL", str(e))
        
        # Test application wrapper functionality
        try:
            from app_integration_wrapper import EnhancedLangGraphApp, get_integration_status
            app = EnhancedLangGraphApp()
            status = get_integration_status()
            
            if status.get('infrastructure_available', False):
                self.log_test("Application Wrapper", "PASS", "Enhanced mode active")
            else:
                self.log_test("Application Wrapper", "PASS", "Fallback mode active")
        except Exception as e:
            self.log_test("Application Wrapper", "FAIL", str(e))
    
    def test_error_handling(self):
        """Test error handling and recovery mechanisms"""
        print("\nüõ°Ô∏è  Testing Error Handling...")
        
        # Test fallback mechanisms
        try:
            from app_integration_wrapper import with_infrastructure_fallback
            
            @with_infrastructure_fallback
            def test_function():
                return "success"
            
            result = test_function()
            if result:
                self.log_test("Fallback Mechanisms", "PASS", "Fallback decorator works correctly")
            else:
                self.log_test("Fallback Mechanisms", "FAIL", "Fallback decorator failed")
        except Exception as e:
            self.log_test("Fallback Mechanisms", "FAIL", str(e))
        
        # Test graceful degradation
        try:
            # Simulate missing component
            original_import = __builtins__.__import__
            
            def mock_import(name, *args, **kwargs):
                if name == 'non_existent_module':
                    raise ImportError("Simulated missing module")
                return original_import(name, *args, **kwargs)
            
            __builtins__.__import__ = mock_import
            
            try:
                import non_existent_module
                self.log_test("Graceful Degradation", "FAIL", "Should have failed import")
            except ImportError:
                self.log_test("Graceful Degradation", "PASS", "Handles missing modules correctly")
            finally:
                __builtins__.__import__ = original_import
                
        except Exception as e:
            self.log_test("Graceful Degradation", "WARN", f"Test inconclusive: {str(e)}")
    
    def test_security_features(self):
        """Test security implementations"""
        print("\nüîí Testing Security Features...")
          # Test rate limiting
        try:
            from enhanced_rate_limiting import EnhancedRateLimiter, RateLimitConfig
            config = RateLimitConfig(
                requests_per_minute=60,
                requests_per_hour=1000
            )
            limiter = EnhancedRateLimiter("test", config)
            
            # Test rate limit check
            result = limiter.check_rate_limit("test_user")
            if result:
                self.log_test("Rate Limiting", "PASS", "Rate limiting functional")
            else:
                self.log_test("Rate Limiting", "WARN", "Rate limiting too restrictive")
        except Exception as e:
            self.log_test("Rate Limiting", "FAIL", str(e))
        
        # Test input validation (if available)
        try:
            from input_security import InputValidator
            validator = InputValidator()
            
            # Test safe input
            safe_result = validator.validate_text("Hello, world!")
            
            # Test potentially dangerous input
            dangerous_result = validator.validate_text("<script>alert('xss')</script>")
            
            if safe_result and not dangerous_result:
                self.log_test("Input Validation", "PASS", "Input validation working correctly")
            else:
                self.log_test("Input Validation", "WARN", "Input validation may have issues")
        except Exception as e:
            self.log_test("Input Validation", "WARN", f"Input validation not available: {str(e)}")
    
    def test_performance_metrics(self):
        """Test performance monitoring and metrics"""
        print("\nüìä Testing Performance Metrics...")
        
        # Test cache performance
        try:
            from cache_manager import CacheManager
            cache = CacheManager()
            
            # Test cache operations
            start_time = time.time()
            cache.set("test_key", "test_value", ttl=60)
            set_time = time.time() - start_time
            
            start_time = time.time()
            value = cache.get("test_key")
            get_time = time.time() - start_time
            
            if set_time < 0.1 and get_time < 0.1:
                self.log_test("Cache Performance", "PASS", 
                            f"Set: {set_time:.4f}s, Get: {get_time:.4f}s")
                self.results["performance"]["cache"] = {
                    "set_time": set_time,
                    "get_time": get_time
                }
            else:
                self.log_test("Cache Performance", "WARN", "Cache operations are slow")
        except Exception as e:
            self.log_test("Cache Performance", "WARN", f"Cache not available: {str(e)}")
          # Test database connection performance
        try:
            from database_connection_pool import DatabaseConnectionPool
            # Use a test database URL
            db_pool = DatabaseConnectionPool("sqlite:///test.db")
            
            start_time = time.time()
            conn = db_pool.get_connection()
            if conn:
                db_pool.return_connection(conn)
            connection_time = time.time() - start_time
            
            if connection_time < 0.5:
                self.log_test("Database Performance", "PASS", 
                            f"Connection in {connection_time:.4f}s")
                self.results["performance"]["database"] = {
                    "connection_time": connection_time
                }
            else:
                self.log_test("Database Performance", "WARN", "Slow database connections")
        except Exception as e:
            self.log_test("Database Performance", "WARN", f"Database not available: {str(e)}")
    
    def test_production_readiness(self):
        """Test production readiness indicators"""
        print("\nüéØ Testing Production Readiness...")
        
        # Test logging configuration
        try:
            import logging
            logger = logging.getLogger("test_logger")
            logger.info("Test log message")
            self.log_test("Logging System", "PASS", "Logging system functional")
        except Exception as e:
            self.log_test("Logging System", "FAIL", str(e))
        
        # Test configuration validation
        try:
            from integrated_config import LangGraphIntegratedConfig
            config = LangGraphIntegratedConfig()
            config.validate()
            self.log_test("Configuration Validation", "PASS", "Configuration is valid")
        except Exception as e:
            self.log_test("Configuration Validation", "WARN", f"Configuration issues: {str(e)}")
        
        # Test file structure
        required_files = [
            "langgraph_enhanced_cli.py",
            "langgraph_enhanced_streamlit.py",
            "integrated_config.py",
            "app_integration_wrapper.py"
        ]
        
        missing_files = []
        for file in required_files:
            if not os.path.exists(file):
                missing_files.append(file)
        
        if not missing_files:
            self.log_test("File Structure", "PASS", "All required files present")
        else:
            self.log_test("File Structure", "WARN", f"Missing files: {missing_files}")
    
    def calculate_phase1_score(self):
        """Calculate overall Phase 1 completion score"""
        total = self.results["summary"]["total_tests"]
        passed = self.results["summary"]["passed"]
        warnings = self.results["summary"]["warnings"]
        
        # Calculate weighted score
        # Pass = 100%, Warning = 75%, Fail = 0%
        score = ((passed * 100) + (warnings * 75)) / (total * 100) * 100 if total > 0 else 0
        
        self.results["summary"]["phase1_score"] = round(score, 1)
        return score
    
    def generate_final_report(self):
        """Generate comprehensive final report"""
        score = self.calculate_phase1_score()
        
        print(f"\nüéØ Phase 1 Final Assessment")
        print("=" * 50)
        print(f"Overall Score: {score}%")
        print(f"Tests Passed: {self.results['summary']['passed']}")
        print(f"Warnings: {self.results['summary']['warnings']}")
        print(f"Failed: {self.results['summary']['failed']}")
        
        # Determine readiness level
        if score >= 95:
            readiness = "üöÄ PRODUCTION READY"
            recommendation = "System is ready for production deployment"
        elif score >= 85:
            readiness = "‚úÖ DEPLOYMENT READY"
            recommendation = "System ready for staging/testing deployment"
        elif score >= 75:
            readiness = "‚ö†Ô∏è  INTEGRATION COMPLETE"
            recommendation = "Integration successful, minor optimizations needed"
        else:
            readiness = "‚ùå NEEDS WORK"
            recommendation = "Significant issues require attention"
        
        print(f"Status: {readiness}")
        print(f"Recommendation: {recommendation}")
        
        # Save detailed report
        report_file = f"phase1_final_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        print(f"üìã Detailed report saved to: {report_file}")
        
        return self.results
    
    def run_comprehensive_validation(self):
        """Run all validation tests"""
        print("üîç Starting Phase 1 Comprehensive Validation")
        print("=" * 60)
        
        self.test_startup_performance()
        self.test_application_integration()
        self.test_error_handling()
        self.test_security_features()
        self.test_performance_metrics()
        self.test_production_readiness()
        
        return self.generate_final_report()

def main():
    """Run Phase 1 final validation"""
    validator = Phase1Validator()
    report = validator.run_comprehensive_validation()
    
    # Return appropriate exit code
    score = report["summary"]["phase1_score"]
    if score >= 85:
        sys.exit(0)  # Success
    else:
        sys.exit(1)  # Needs improvement

if __name__ == "__main__":
    main()
